---
title: Dataset
description: Dataset
---

import { ImageZoom } from "fumadocs-ui/components/image-zoom";
import { File, Folder, Files } from "fumadocs-ui/components/files";
import Node from "@/app/_components/node";
import { Callout } from "fumadocs-ui/components/callout";
import DatasetFormImage from "@/public/assets/node_form_dataset.png";
import PatchesImage from "@/public/assets/patches.png";
import RandomSamplitImage from "@/public/assets/random_sampling.png";

The dataset component is a [node](/docs/concepts/workspace#nodes) responsible
for creating a dataset for training the network. It creates a file containing a
collection of images and labels stored in an apropriate format for deep learning
data processing that will be used to train a network. When clicked, the dataset
will open a panel with a form to be filled in with the necessary information to
create a dataset. Let's go through the form fields:

<ImageZoom
  alt="Dataset Form"
  src={DatasetFormImage}
  className="!my-0 rounded-sm"
  priority
/>
_Form fields of the the dataset node._

## Images

First, you’ll need a set with original (grayscale) and segmented (label) images
with the same dimensions. Those images don’t need to have the same name, only to
be in the same order in the list of images when creating a dataset. This
organized data will be used to train with predefined parameters, allowing the
computer to learn by itself by recognizing patterns in various processing
layers.

**Image**: You can add files in tiff or raw extensions. This is the grayscale
image.

**Label**: the dataset must include n distinct classes, with labels assigned
sequentially from 0 to n-1 .

**Weight Map**: It is used when you want to make the classifier understand that
it must focus on specific areas. The weight map is an image whose voxel
intensities make the network training penalize the classification error
proportionally to the value of the voxel. Higher values will force the network
to improve classification for certain voxels. When certain materials/objects are
small, for instance, it might be interesting to provide a weight image with
higher values for those objects. If you do not provide a weight image, the
network training will assume that the image has uniform weight equal to 1.0.

**Note**: In case of raw images, it is important to point out that, in order to
infer the image properties, the file must contain necessary information (size
XxYxZ and bitdepth and data type), for example:

<Files>
  <File name="this_is_my_float_file_200x200x100_16float.raw" />
  <File name="this_is_my_int_file_200x200x100_32bits.raw" />
  <File name="this_is_my_uint_file_200x200x100_16bits.raw" />
</Files>

<Callout type="warn">
  Keep in mind that the label must have enough visual field in order to
  understand the shape and texture of your object.
</Callout>

## Classes

Number of classes in the label file.

## Sample Size

The number of random patches extracted from a strategy (uniform avaible only)
used to create the train dataset, example below shows a blue cube patch
extracted to a pink cube for training. The size of the dataset depends on the
sample size. The larger the sample size, the heavier the file will become.

<ImageZoom
  alt="Patches"
  src={PatchesImage}
  className="!my-0 rounded-sm"
  priority
/>

## Strategy

Uniform only. It means equal probability of extraction/selection of patches for
training. It randomly extracts patches over a uniform distribution, but that
doesn't mean it's a perfect grid.

## Patch Size

The network always works with a small crop of the image henceforth denoted
patch, this parameter adjusts its size.

<ImageZoom
  alt="Random Sampling"
  src={RandomSamplitImage}
  className="!my-0 rounded-sm"
  priority
/>
_The uniform window sampler first computes a set of all feasible spatial locations
(so that the windows are always within the image) and randomly draws samples from
the set._
